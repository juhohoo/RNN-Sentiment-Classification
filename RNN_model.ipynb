{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c31d651-12c8-4004-837e-d1ca642e7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import spacy\n",
    "import regex as re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchtext import vocab\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6e32da-c08a-4423-9595-126ca7da7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants - Add here as you wish\n",
    "N_EPOCHS = 5\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "TRAIN_FILE = '../data/sent140.train.mini.csv'\n",
    "DEV_FILE   = '../data/sent140.dev.csv'\n",
    "TEST_FILE  = '../data/sent140.test.csv'\n",
    "\n",
    "TRAIN_BS = 32\n",
    "DEV_BS   = 32\n",
    "TEST_BS  = 32\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12f7e83-96b7-44fa-a2fb-c61239ea0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilary functions for data preparation\n",
    "tok = spacy.load('en_core_web_sm',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in tok(tweet_clean(s))]\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305d866a-3eeb-473a-ac21-fd99c167f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def evaluate(model, loader, criterion):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() # set model for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            #WRITE CODE HERE\n",
    "            inputs, labels = batch['inputs'], batch['labels'] # labels=torch.Size([32])\n",
    "            outputs = model(inputs) #torch.Size([1, 32, 2])\n",
    "            outputs=torch.squeeze(outputs) # remove dim=1\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            pass\n",
    "\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46f4525-bd94-4971-9111-988507990c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4948ed2c-27ab-47bc-a7e1-84244e85dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #WRITE CODE HERE\n",
    "        \n",
    "        # Embedding layer:\n",
    "        self.emb = nn.Embedding.from_pretrained(glove_embeddings)\n",
    "        \n",
    "        # Recurrent layer:\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim) \n",
    "        \n",
    "        # LSTM:\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Fully connected layer:\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, lengths=None):\n",
    "        #WRITE CODE HERE\n",
    "        inputs = self.emb(torch.tensor(inputs))\n",
    "        output, hidden = self.rnn(inputs)\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2964e278-fff6-497c-919b-b66753876eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ce21b6-2aac-4a6b-a162-8366c1f0247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/sent140.train.mini.csv\n",
      "Loading ../data/sent140.dev.csv\n",
      "Loading ../data/sent140.test.csv\n",
      "Getting datasets\n",
      "Getting loaders\n"
     ]
    }
   ],
   "source": [
    "# Preparation dataset\n",
    "\n",
    "train_loader, dev_loader, test_loader, glove_embeddings = data.get_dataset(\n",
    "                tokenizer,\n",
    "                TRAIN_FILE,\n",
    "                DEV_FILE,\n",
    "                TEST_FILE,\n",
    "                TRAIN_BS,\n",
    "                DEV_BS,\n",
    "                TEST_BS,\n",
    "                EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ff76c8-6931-42ad-b158-abe208a35405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer and loss function\n",
    "# hidden_dim = number of features in RNN layer\n",
    "# output_dim = number of classes = 2 (Negative vs. Positive)\n",
    "\n",
    "model = RNN(embedding_dim=EMBEDDING_DIM, hidden_dim=2, output_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss() # Takes logits as input (raw network output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a00616-1472-478a-bbe6-4f50a1fb585a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (emb): Embedding(1193515, 200)\n",
       "  (rnn): RNN(200, 2)\n",
       "  (fc): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9842e2c9-fb10-4881-9341-ffe6c4cbfb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juho-local\\AppData\\Roaming\\Python\\Python38\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "<ipython-input-6-12960cc53d7e>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = self.emb(torch.tensor(inputs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 19s\n",
      "\tTrain Loss: 0.709 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 0.00%\n",
      "Epoch 1\n",
      "Epoch: 02 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.696 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 0.00%\n",
      "Epoch 2\n",
      "Epoch: 03 | Epoch Time: 2m 12s\n",
      "\tTrain Loss: 0.695 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 0.00%\n",
      "Epoch 3\n",
      "Epoch: 04 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.694 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 0.00%\n",
      "Epoch 4\n",
      "Epoch: 05 | Epoch Time: 2m 14s\n",
      "\tTrain Loss: 0.694 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# --- Train Loop ---\n",
    "\n",
    "print('Training')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch {epoch}')\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    correct = 0  \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        #WRITE CODE HERE\n",
    "        inputs, labels = batch['inputs'], batch['labels'] # labels=torch.Size([32])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #torch.Size([1, 32, 2])\n",
    "        outputs=torch.squeeze(outputs) # remove dim=1\n",
    "        \n",
    "        \"\"\"\n",
    "        Tried to implement accuracy calculation method, but got some errors about tensor sizes:\n",
    "        \n",
    "        probabilities = F.softmax(outputs)\n",
    "        predictions = probs.argmax(dim=1)\n",
    "        epoch_acc += torch.sum(predictions == labels).item()\n",
    "        \"\"\"\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss, train_acc = (epoch_loss / len(train_loader), epoch_acc / len(train_loader)) \n",
    "    valid_loss, valid_acc = evaluate(model, dev_loader, criterion)\n",
    "            \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48817559-a739-4e68-b29e-4bff107d825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-12960cc53d7e>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = self.emb(torch.tensor(inputs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss:  0.6952377961466487\n",
      "Computing time (s):  66.46612405776978\n"
     ]
    }
   ],
   "source": [
    "# --- Test model ---\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'\\tTest Loss: ', test_loss)\n",
    "end_time = time.time()\n",
    "print('Computing time (s): ', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e2f92-b884-46e8-8668-51e57235545a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
